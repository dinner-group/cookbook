{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75972dd7-35d3-4ca3-ad37-58f22b903511",
   "metadata": {},
   "source": [
    "# Attention is all you need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f4946-062a-41fb-8421-48e7b3ca39c5",
   "metadata": {},
   "source": [
    "Adapted from https://colab.research.google.com/github/crazycloud/dl-blog/blob/master/_notebooks/2021_03_18_Transformers_Multihead_Self_Attention_Explanation_%26_Implementation_in_Pytorch.ipynb and http://nlp.seas.harvard.edu/annotated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f378b1-3ff2-4668-a76c-0471445780b0",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3a7c590-b974-4292-ba0f-99451378d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bbf944f-14d3-4b5f-bf11-c7198013da65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f649cfa-6f15-4923-b8af-91ae8cb71709",
   "metadata": {},
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e54a8fe-ce79-4803-9fe6-4e8d2c56bc4c",
   "metadata": {},
   "source": [
    "$d_E$ - embedding dim: The word embedding dimension and also the dimension for each key, value and query vectors. We assume each word is a $d_E$ dimension vector and we use a Linear Layer to Transform the token vector in to $K, V, Q$ vector of $d_k$ dimension. The reason to transform it into $d_E$ dimension is related to Multiheads where we calculate attention for multiple heads. We will add the multihead after implementing a simple self attention module\n",
    "\n",
    "$n$ - sequence length: Maximum number of vectors to process at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb490a7-c949-4a03-9fb3-1c4e9e9bec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5 # 5 words sequence length (in practice this would include padding)\n",
    "d_E = 256 # dimension of word embedding\n",
    "d_k = 256 # dimension of key/query projection\n",
    "d_v = d_k # dimension of value projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0252c79c-c84d-4565-9c6e-aeee5b3a3e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0c382f6490>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b408c97-bbc4-4569-963c-5cbfc2fd1029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "# generate random sample data\n",
    "n_B = 10\n",
    "x = torch.randn(n_B, n, d_E) # (n, d_E)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe1a756-96b0-4f84-b63b-650c0d6c729a",
   "metadata": {},
   "source": [
    "If we consider batch dimension as well the input will be of shape $(n_B, n, d_E)$\n",
    "\n",
    "Transform input sequence $X$ into [key, query, value] of dimension $d_k$ (assume $d_k = d_v$ for now)\n",
    "$x \\rightarrow q, k, v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd6c2cf-e0c4-44bc-945f-5af8856da7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection matrices Q, K, V\n",
    "query_lin = nn.Linear(d_E, d_k, bias=False) \n",
    "key_lin = nn.Linear(d_E, d_k, bias=False)\n",
    "val_lin = nn.Linear(d_E, d_v, bias=False)\n",
    "\n",
    "q = query_lin(x) \n",
    "k = key_lin(x) \n",
    "v = val_lin(x)\n",
    "# what is shape of q, k, v?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5efeedb2-0af4-41f4-95c7-8a75924be0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 256]) torch.Size([10, 5, 256]) torch.Size([10, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "print(q.shape, k.shape, v.shape) # (n_B, n, d_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9228bef0-6256-4039-9772-81f6d075e9f7",
   "metadata": {},
   "source": [
    "Now compute attention scores with scalled dot product attention: \n",
    "$$\\mathsf{softmax}_j\\left(\\frac{(XQ)(XK)^\\top}{\\sqrt{d_k}}\\right)(XV) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc06066-b54b-4f46-bd61-834a5e1ff65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e80d73-854c-48ea-b953-dd49bae2f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, attn_score = attention(q, k, v)\n",
    "# what is shape of x, attn_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba659fdd-a51a-4a96-a6c7-b06b5f20af24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 256]) torch.Size([10, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, attn_score.shape) # (n_B, n, d_v) and (n_B, n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f487680d-15db-484a-8dd3-54e7288f2675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0b97715210>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(attn_score[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096aa7a9-9bbb-4d1d-a7a1-ceb538d7ec0f",
   "metadata": {},
   "source": [
    "### Same thing as a module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d0e51-42d0-4072-9773-3610be44f850",
   "metadata": {},
   "source": [
    "https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f05af223-3722-41cb-bbf1-b51ce73501fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super().__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(-1, -2)  # unnormalized attention weights    \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / self.d_out_kq**0.5, dim=-1\n",
    "        )\n",
    "        \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59032d26-8f1c-4f1f-9889-f0df7e906221",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn = SelfAttention(d_E, d_k, d_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3681a17-4de6-4344-a4a8-9d3c7da97ca3",
   "metadata": {},
   "source": [
    "## Multi-head self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dc5602-9f84-411c-bcaf-5ad83aefb571",
   "metadata": {},
   "source": [
    "Multi-head self attention is defined as\n",
    "$$\\mathsf{MultiHead}(X) = [\\mathsf{head}_1,\\dots,\\mathsf{head}_h]W^O$$\n",
    "where \n",
    "$$ \\mathsf{head}_i = \\mathsf{Attention}(XQ_i, XK_i, XV_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8ee328f-cced-46eb-8d64-8b42106c1172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpler version\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [SelfAttention(d_in, d_out_kq, d_out_v) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42e5e4d0-d6c6-4480-954d-b02341e8d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        \"Implements Figure 2\"\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        print(self.linears[0](query).shape)\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        # QUESTION: what would print(query.shape, key.shape, value.shape) give?\n",
    "        print(query.shape, key.shape, value.shape)\n",
    "        # what about before the view? i.e. the output of lin(x)?\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(query, key, value)\n",
    "        # QUESTION: what would print(x.shape) give?\n",
    "        print(x.shape)\n",
    "\n",
    "        # 3) \"Concat\" using a view\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        # QUESTION: what would print(x.shape) give?\n",
    "        print(x.shape)\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        \n",
    "        # apply a final linear\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a99fb56c-a64c-4310-ab1f-886323e18856",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out = 256\n",
    "h = 8\n",
    "mha = MultiHeadedAttention(h, d_out) # what is d_v if d_out = 256 and h = 8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e10d137b-64df-4a5f-be2a-344e5ac03d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 256])\n",
      "torch.Size([10, 8, 5, 32]) torch.Size([10, 8, 5, 32]) torch.Size([10, 8, 5, 32])\n",
      "torch.Size([10, 8, 5, 32])\n",
      "torch.Size([10, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "x = mha(x, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bceba001-1afe-4216-a153-5be2e21c88e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5797b0-7f7d-4b14-9378-e7cf582ddeb7",
   "metadata": {},
   "source": [
    "Answers:\n",
    "- $(n_B, n, h*d_k)$\n",
    "- $(n_B, h, n, d_k)$ or $n_B, h, n, d_v)$\n",
    "- $(n_B, h, n, d_v)$\n",
    "- $(n_B, n, h *d_v)$\n",
    "- $(n_B, n, d_{model})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38fb44b-3942-47a8-a7ba-cbcb0b50f781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
